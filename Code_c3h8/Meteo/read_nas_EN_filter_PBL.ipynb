{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import metpy\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import xarray as xr\n",
    "import metpy.calc as mpcalc\n",
    "from metpy.units import units\n",
    "from metpy.calc import dewpoint_from_relative_humidity\n",
    "from metpy.calc import specific_humidity_from_dewpoint\n",
    "from pint import UnitRegistry\n",
    "ureg = UnitRegistry()\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract epoch from the first few rows of the CSV file\n",
    "def extract_epoch_from_header(file):\n",
    "    # Read the first few lines to find the epoch (assuming it's in a comment or a header)\n",
    "    with open(file, 'r') as f:\n",
    "        skiprow = f.readline()\n",
    "        skiprow = skiprow.strip().split()[0]\n",
    "        for line in f:\n",
    "            if \"startdate:\" in line.lower():  # Assuming the word 'epoch' is present in the line\n",
    "                # Extract the epoch date (this assumes the date is the second item in the line)\n",
    "                epoch = line.strip().split()[1]\n",
    "                return int(skiprow)-1, epoch[0:4]+\"-\"+epoch[4:6]+\"-\"+epoch[6:8]+\" \"+epoch[8:10]+\":\"+epoch[10:12]+\":\"+epoch[12:14]\n",
    "    return None  # Return None if no epoch is found\n",
    "\n",
    "# Function to load and adjust time for each CSV\n",
    "def load_and_adjust_time(file):\n",
    "    # Try to extract the epoch from the header or a specific column\n",
    "    nskiprows,epoch = extract_epoch_from_header(file)\n",
    "   \n",
    "    if epoch is None:\n",
    "        raise ValueError(f\"Could not find epoch for file: {file}\")\n",
    "   \n",
    "    # Load the actual data (skipping header if necessary)\n",
    "    df = pd.read_csv(file, skiprows=nskiprows,sep=\"\\\\s+\")  # Adjust skiprows based on where the data starts\n",
    "   \n",
    "    # Convert 'time' column to a datetime, assuming it's in days\n",
    "    # Adjust time column to start from the epoch time found in the file\n",
    "    df['starttime_dt'] = pd.to_timedelta(df['starttime'], unit='D') + pd.Timestamp(epoch)\n",
    "   \n",
    "    # Return the adjusted DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a working directory till NMVOC folder\n",
    "working_dir = \"C:\\\\Year1\\\\\"\n",
    "# Find all CSV files (change the path if nedeed)\n",
    "rh_files = glob.glob(working_dir+\"NMVOC\\\\Meteo_ebas_CMN\\\\Ebas_240920_1134\\\\*.nas\")\n",
    "print(rh_files)\n",
    "\n",
    "# this function reads and adjust times for all CSV files\n",
    "dfs = []\n",
    "for file in rh_files:\n",
    "    df = load_and_adjust_time(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "# concatenates all DataFrames (based on time)\n",
    "rh_df = pd.concat(dfs).sort_values('starttime_dt').reset_index(drop=True)\n",
    "\n",
    "# Show combined DataFrame\n",
    "print(rh_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a working directory till NMVOC folder\n",
    "working_dir = \"C:\\\\Year1\\\\\"\n",
    "# Find all CSV files (change the path if nedded)\n",
    "temperature_files = glob.glob(working_dir+\"NMVOC\\\\Meteo_ebas_CMN\\\\Ebas_240920_1140\\\\*.nas\")\n",
    "print(temperature_files)\n",
    "\n",
    "# this function reads and adjust times for all CSV files\n",
    "dfs = []\n",
    "for file in temperature_files:\n",
    "    df = load_and_adjust_time(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "# concatenates all DataFrames (based on time)\n",
    "temperature_df = pd.concat(dfs).sort_values('starttime_dt').reset_index(drop=True)\n",
    "\n",
    "# Show combined DataFrame\n",
    "print(temperature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a working directory till NMVOC folder\n",
    "working_dir = \"C:\\\\Year1\\\\\"\n",
    "# Find all CSV files (change the path if nedded)\n",
    "temperature_files = glob.glob(working_dir+\"NMVOC\\\\Meteo_ebas_CMN\\\\Ebas_240923_1438\\\\*.nas\")\n",
    "print(temperature_files)\n",
    "\n",
    "# this function reads and adjust times for all CSV files\n",
    "dfs = []\n",
    "for file in temperature_files:\n",
    "    df = load_and_adjust_time(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "# concatenates all DataFrames (based on time)\n",
    "p_df = pd.concat(dfs).sort_values('starttime_dt').reset_index(drop=True)\n",
    "\n",
    "# Show combined DataFrame\n",
    "print(p_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a working directory till NMVOC folder\n",
    "working_dir = \"C:\\\\Year1\\\\\"\n",
    "# Find all CSV files (change the path if nedeed)\n",
    "wind_s_files = glob.glob(working_dir+\"NMVOC\\\\Meteo_ebas_CMN\\\\Ebas_240920_1137\\\\*.nas\")\n",
    "print(wind_s_files)\n",
    "\n",
    "# this function reads and adjust times for all CSV files\n",
    "dfs = []\n",
    "for file in wind_s_files:\n",
    "    df = load_and_adjust_time(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "# concatenates all DataFrames (based on time)\n",
    "wind_s_df = pd.concat(dfs).sort_values('starttime_dt').reset_index(drop=True)\n",
    "\n",
    "# Show combined DataFrame\n",
    "print(wind_s_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=temperature_df\n",
    "df [\"RH\"] = rh_df.rH\n",
    "df [\"flag_RH\"] = rh_df.flag_rH\n",
    "df [\"flag_P\"] = p_df.flag_P\n",
    "df [\"P\"] = p_df.P\n",
    "df [\"flag_wind_s\"] = wind_s_df.flag_wind_s\n",
    "df [\"wind_s\"] = wind_s_df.wind_s\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove lines depending on flags\n",
    "df=temperature_df\n",
    "df [\"RH\"] = rh_df.rH\n",
    "df [\"flag_RH\"] = rh_df.flag_rH\n",
    "df [\"flag_P\"] = p_df.flag_P\n",
    "df [\"P\"] = p_df.P\n",
    "df [\"flag_wind_s\"] = wind_s_df.flag_wind_s\n",
    "df [\"wind_s\"] = wind_s_df.wind_s\n",
    "df = df[df.flag_T != 0.999]\n",
    "df = df[df.flag_RH != 0.999]\n",
    "df = df[df.flag_P != 0.999]\n",
    "df = df[df.flag_wind_s != 0.999]\n",
    "df = df[df.flag_T != 0.899]\n",
    "df = df[df.flag_RH != 0.899]\n",
    "df = df[df.flag_P != 0.899]\n",
    "df = df[df.flag_wind_s != 0.899]\n",
    "df = df[df.flag_wind_s != 0.456]\n",
    "df = df[df.flag_T != 0.260]\n",
    "df = df[df.flag_RH != 0.260]\n",
    "df = df[df.flag_P != 0.260]\n",
    "df = df[df.flag_wind_s != 0.260]\n",
    "df = df[df.flag_T != 0.259]\n",
    "df = df[df.flag_RH != 0.259]\n",
    "df = df[df.flag_P != 0.259]\n",
    "df = df[df.flag_wind_s != 0.259]\n",
    "df = df[df.flag_T != 0.256]\n",
    "df = df[df.flag_RH != 0.256]\n",
    "df = df[df.flag_P != 0.256]\n",
    "df = df[df.flag_wind_s != 0.256]\n",
    "df.index = df[\"starttime_dt\"]\n",
    "#remove useless columns\n",
    "df = df.drop(columns=[\"flag_P\"])\n",
    "df = df.drop(columns=[\"flag_RH\"])\n",
    "df = df.drop(columns=[\"flag_T\"])\n",
    "df = df.drop(columns=[\"flag_wind_s\"])\n",
    "df = df.drop(columns=[\"starttime\"])\n",
    "df = df.drop(columns=[\"endtime\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop for rounding to the hours\n",
    "df[\"starttime_dt\"] = [i.round(\"H\") for i in df[\"starttime_dt\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper columns with  hour, month, year\n",
    "df['hour'] = df['starttime_dt'].dt.hour\n",
    "df['month'] = df['starttime_dt'].dt.month\n",
    "df['year'] = df['starttime_dt'].dt.year\n",
    "df['date'] = df['starttime_dt'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dewpoint from T and RH\n",
    "dewpoint=dewpoint_from_relative_humidity(df[\"T\"].values*units.degC, df[\"RH\"].values*units.percent)\n",
    "dewpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specific humidity from temperature and dewpoint\n",
    "SH=specific_humidity_from_dewpoint(df[\"P\"].values*units.hPa, dewpoint).to('g/kg')\n",
    "df [\"SH\"] = SH\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date column to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Function to determine season\n",
    "def get_season(date):\n",
    "    month = date.month\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Fall'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#season\n",
    "seasons = [1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 1]\n",
    "month_to_season = dict(zip(range(1,13), seasons))\n",
    "month_to_season \n",
    "S=df.starttime_dt.dt.month.map(month_to_season) \n",
    "df['season']=S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Extract year and season\n",
    "df['year'] = df['date'].dt.year\n",
    "df['season'] = df['date'].apply(get_season)\n",
    "\n",
    "# Group by year and season and calculate mean value (or sum, etc.)\n",
    "grouped = df.groupby(['season', 'year'])['SH'].mean().unstack()\n",
    "\n",
    "# Reorder the seasons\n",
    "season_order = ['Winter', 'Spring', 'Summer', 'Fall']\n",
    "grouped = grouped.reindex(season_order)\n",
    "\n",
    "# Plot for different years\n",
    "grouped.plot(kind='line', marker='o', figsize=(10, 6))\n",
    "\n",
    "# Customize plot\n",
    "plt.title('Value by Season for Different Years')\n",
    "plt.ylabel('SH [g/kg]')\n",
    "plt.xlabel('Season')\n",
    "plt.xticks(range(len(season_order)), season_order)\n",
    "plt.legend(title='Year')\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter the data to include only rows where the second column's values fall between the 25th and 75th percentiles.\n",
    "#Plot the boxplot for the filtered data.\n",
    "\n",
    "#import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# Example DataFrame\n",
    "#data = {\n",
    " #   'column1': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],  # Column to filter (for percentiles)\n",
    "  #  'column2': [5, 15, 25, 35, 45, 55, 65, 75, 85, 95]     # Column to plot\n",
    "#}\n",
    "\n",
    "\n",
    "# Step 1: Calculate the 25th and 75th percentiles of 'column1'\n",
    "q25_ws = df['wind_s'].quantile(0.25)\n",
    "q75_ws = df['wind_s'].quantile(0.75)\n",
    "q25_SH = df['SH'].quantile(0.25)\n",
    "q75_SH = df['SH'].quantile(0.75)\n",
    "\n",
    "# Step 2: PBL: Filter the Data to include only rows where WS values are <= 25th and SH >= 75th percentiles\n",
    "PBL_df = df[(df['wind_s'] <= q25_ws) & (df['SH'] >= q75_SH)]\n",
    "# Step 2: FT (free troposphere): Filter the Data to include only rows where WS values are >= q75_ws and SH<= q25_SH percentiles\n",
    "FT_df = df[(df['wind_s'] >= q75_ws) & (df['SH'] <= q25_SH)]\n",
    "\n",
    "#descriptive statistics of PBL and FT regime occurrance in 24h and month\n",
    "#PBL_df=(df.groupby(['month']).agg({'SH': ['mean','min', 'max', 'std', 'count'] }))\n",
    "\n",
    "# Step 4: Create a boxplot for 'column2' using the filtered DataFrame\n",
    "#plt.boxplot(PBL_df['column2'])\n",
    "#plt.title('Boxplot of Column2 for Values in PBL')\n",
    "#plt.ylabel('Values in Column2')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PBL_df.groupby(['month']).agg({'SH': ['mean','min', 'max', 'std', 'count'] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FT_df.groupby(['month']).agg({'SH': ['mean','min', 'max', 'std', 'count'] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the 25th and 75th percentiles of 'column1'\n",
    "q25_ws = df['wind_s'].quantile(0.25)\n",
    "q75_ws = df['wind_s'].quantile(0.75)\n",
    "q25_SH = df['SH'].quantile(0.25)\n",
    "q75_SH = df['SH'].quantile(0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_s=df['wind_s']\n",
    "SH=df['SH']\n",
    "def _conditions3(wind_s , SH):\n",
    "    rank = (     \n",
    "          \"PBL\" if (wind_s <= q25_ws) & (SH >= q75_SH)\n",
    "    else  \"FT\"  if (wind_s >= q75_ws) & (SH <= q25_SH)\n",
    "    else  \"Und\"  \n",
    "    )\n",
    "    \n",
    "    return rank\n",
    "# Vectorize the function\n",
    "func = np.vectorize(_conditions3)\n",
    "# Create a new column based on the function\n",
    "df[\"Atmos\"] = func(wind_s, SH)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#occurrence in each hour\n",
    "# Step 2: Group by 'Month' and 'Flag' and count the occurrences\n",
    "flag_counts = df.groupby(['hour', 'Atmos']).size().reset_index(name='Count')\n",
    "\n",
    "# Step 3: Plot the scatter plot using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=flag_counts, x='hour', y='Count', hue='Atmos', s=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Group by 'hour' and Flag in 'Atmos' and count the occurrences\n",
    "flag_counts = df.groupby(['hour', 'Atmos']).size().reset_index(name='Count')\n",
    "\n",
    "# Step 3: Calculate the total number of items in the DataFrame (total flags)\n",
    "total_count = len(df)\n",
    "\n",
    "# Step 4: Calculate the percentage for each combination of Month and Flag in 'Atmos'\n",
    "flag_counts['Percentage'] = (flag_counts['Count'] / total_count) * 100\n",
    "\n",
    "# Display the result\n",
    "print(flag_counts[['hour', 'Atmos', 'Count', 'Percentage']])\n",
    "\n",
    "# Display the result\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=flag_counts, x='hour', y=flag_counts['Percentage'], hue='Atmos', s=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#“FT”, PBL-representative data in 24h\n",
    "# Step 2: Group by 'hour' and Flag in 'Atmos' and count the occurrences\n",
    "flag_counts = df.groupby(['hour', 'Atmos']).size().reset_index(name='Count')\n",
    "\n",
    "# Step 3: Calculate total counts per each flag across all hours\n",
    "total_flag_counts = df.groupby('Atmos').size().reset_index(name='Total Count')\n",
    "\n",
    "# Step 4: Merge the flag counts with the total counts\n",
    "flag_counts = pd.merge(flag_counts, total_flag_counts, on='Atmos')\n",
    "\n",
    "# Step 5: Calculate the percentage for each combination of Month and Flag based on the total count per flag\n",
    "flag_counts['Percentage'] = (flag_counts['Count'] / flag_counts['Total Count']) * 100\n",
    "\n",
    "# Display the result\n",
    "print(flag_counts[['hour', 'Atmos', 'Count', 'Total Count', 'Percentage']])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=flag_counts, x='hour', y='Percentage', hue='Atmos', s=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clacluate percentage of PBL occurence between 10 and 15 out of the PBL occurance h24\n",
    "filtered_df = flag_counts[(flag_counts['Atmos'] == 'PBL') & (flag_counts['hour'].isin([10, 11, 12, 13, 14, 15]))]\n",
    "\n",
    "# Step 7: Sum the percentages\n",
    "total_percentage_PBL = filtered_df['Percentage'].sum()\n",
    "\n",
    "# Display the result\n",
    "print(total_percentage_PBL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clacluate percentage of FT occurence between 20 and 6 out of the PBL occurance h24\n",
    "filtered_df = flag_counts[(flag_counts['Atmos'] == 'FT') & (flag_counts['hour'].isin([20, 21, 22, 23, 0, 1,  2, 3, 4, 5, 6]))]\n",
    "\n",
    "# Step 7: Sum the percentages\n",
    "total_percentage_FT = filtered_df['Percentage'].sum()\n",
    "\n",
    "# Display the result\n",
    "print(total_percentage_FT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#occurrence in each month \n",
    "# Step 2: Group by 'Month' and 'Flag' and count the occurrences\n",
    "flag_counts_m = df.groupby(['month', 'Atmos']).size().reset_index(name='Count')\n",
    "\n",
    "# Step 3: Plot the scatter plot using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=flag_counts_m, x='month', y='Count', hue='Atmos', s=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##how flags are distributed across months in relation to the entire dataset\n",
    "# Step 2: Group by 'Month' and 'Flag in 'Atmos'' and count the occurrences\n",
    "flag_counts_m = df.groupby(['month', 'Atmos']).size().reset_index(name='Count')\n",
    "\n",
    "# Step 3: Calculate the total number of items in the DataFrame (total flags)\n",
    "total_count = len(df)\n",
    "\n",
    "# Step 4: Calculate the percentage for each combination of Month and Flag in 'Atmos'\n",
    "flag_counts_m['Percentage'] = (flag_counts_m['Count'] / total_count) * 100\n",
    "\n",
    "# Display the result\n",
    "print(flag_counts_m[['month', 'Atmos', 'Count', 'Percentage']])\n",
    "\n",
    "# Display the result\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=flag_counts_m, x='month', y=flag_counts_m['Percentage'], hue='Atmos', s=100)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
